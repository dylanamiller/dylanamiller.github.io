---
title: DQN in 130 Lines
author: Dylan Miller
layout: post
---
## Before I begin, I recommend going to my (github)[https://github.com/dylanamiller/dqn_in_130_lines] page to see the full notebook that goes along with this post. If this post seems odd, it is because I wrote the notebook first and just deleted a bunch of stuff to get this post.

I will quickly go through the details of the (DQN algorithm)[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf]. For starters, DQN, or Deep Q Network, is (Q Learning)[see https://dylanamiller.github.io/] with extra bells and whistles; the main bell and/or whistle being the use of a neural network as the function approximator - interestibly enough, doing this for Q Learning actuall causes the algorithm to have (no gaurentee of convergence)[https://www.mit.edu/~jnt/Papers/J063-97-bvr-td.pdf] despite the algorithm's success. Along with using a neural network of course, come certain alterations to the Q Learning algorithm that are required in order to make it manageable. But do not let these things obscure what is really going on: Q Learning.

## **Experience Replay**

To begin, we define the Experience Replay Buffer. This is the fancy name for the user defined data structure that will hold the transition tuples our algorithm will collect as it attempts to navigate the environment. DQN (even though it is really just Q Learning, I will revert to saying DQN as it saves space) is an off-policy algorithm. This means, that the policy being used to define the agent's behavior is not the one we are improving; the reason for this will be apparent in a bit. This means that we do not need to use current, by which I mean from the agent's history within the current update period, experience in order to train our agent. We can instead choose to use experience from the entire history or training. This is what makes off-policy algorithms more sample efficient that on policy algorithms.

The buffer has a function to add experience tuples (i.e. state, action, reward, next state, action). It also needs a funtion to randomly sample batches of experience tuples from our buffer in order to perform our update.

## **Policy Definition**

To help our agent choose actions, we will use an epsilon greedy policy. This means, that most of the time we will greedily choose the action that the highest Q value, but epsilon percent of the time, we will choose an action randomly. It is common to decay the epsilon over the duration of training, much like you would the learning rate.

This is probably the most common way for DQN to handle the exploration vs exploitation dilema. In case you are not familiar with this concept (first I would go learn some Reinforcement Learning (RL) basics before trying to learn RL with neural networks, but I won't judge), exploration is how much our agent should look for new experience, and exploitation is how much our agent should prioritize behavior it already knows to be good (at least compared to its experience so far, which may or may not be optimal given the underlying environment dynamics); any good RL agent must find a way to balance these.

## **Training**

Finally, we have the training loop. First, I will walk through the Q Learning algorithm, then I will point out something about why the algorithm does what it does and the result of doing this, and finally I will explain the changes made in order to accomodate the neural network.

The first thing we have to do, is prime out buffer as described above. We only have to do this once for the entire run of the algorithm, because the agent will accumulate experience.

At the start of the episode, we get a new state. We use this, to get our Q values that correspond to the value of taking action in that state. With our Q values in hand, we use the epsilon greedy policy to choose an action. We use the chosen action to step the environment forward and get the reward and next state. This next state is run through the network to get its Q values. This time, instead of using our epsilon greedy policy to choose the action, we will simply choose the action with the highest Q value. 

This is our entire experience tuple. So we add (state, action, reward, next state and next action) to our buffer. At this point, we will set the state to be what is currently the next state and repeat the experience tuple collection process. 

At the end of the episode, we will randomly sample a batch from our buffer and perform our update. Now, the Q learning update is $R + \gamma max_{a \in A}Q_{\theta}(S_{t+1}, a) - Q_{\theta}(S_{t}, A_{t})$. This is very similar to the TD error, and it intuitively tells us that the update is the difference between what happened and what we expected to happen. 

And that is the algorithm. Or at least that is the Q learning algorithm. I will explain necessary changes in a moment. First however, I want to make a note of the second term in our update, $\gamma max_{a \in A}Q_{\theta}(S_{t+1}, a)$:

$\gamma$ is the discount factor. This tells us by how much we wish to say that future rewards are not as important as the current reward. 1 corresponds to no discount and says that we want all rewards considered equally and 0 says that only the current reward matter. The choice depends on the problem, but for toy problems such as CartPole, a value like 0.9 or 0.99 are fairly standard choices. 

The more important part of this term, is $max_{a \in A}Q_{\theta}(S_{t+1}, a)$. $R$ plus this term could describe a possible Q value from $S_{t}$ for a different policy. So, given a different policy, say with parameters $\theta'$, this could be written as $Q_{\theta'}(S_{t}, A_{t})$, which looks an awful lot like the final term of the update. It is this fact that makes Q Learning an off-policy algorithm. Rather than use the agent's behavior defining policy, epsilon greedy, to choose the action, we are using the max action. The result is that the agent is attempting the estimate the optimal policy. Cool strategy. 

This is also why we can use experience from any point in training. In RL, the state distribution underlying the agent's behavior is not stationary. As the agent learns, its probability of ending up in states changes as it action preference changes. For on-policy algorithms, where the agent is updating the same policy it is using to choose actions, that means that its policy is moving around underneath it quite a bit, so it can only use experience from its current update period. In Q Learning however, we are trying to estimate the optimal policy, which does not change. So, all data points converge toward it and will help with our updates.

Now, for the required (not really, but they make it work better) neural network changes. Since there is a considerable amount of noise that results from using a nonlinear function approximator and, in this case, constantly comparing Q values against a changing set of Q values, we will actually want to define two neural networks rather than one. We will not train our second network, but freeze it with the weights of the first network from a given update period. While this network will be kept frozen, we will occasionally set it to match the current weights of our training network. We will use this second network as our target (i.e. the term added to the reward). By keeping its weights, and therefore Q values, largely stationary, we are able to remove some noise from our updates. Imagine trying to throw pebbles in a jar, but the jar were to move every time you threw. It would be very difficult. By keeping it still, you will get better at aiming at the jar.

We will call the parameters of this second network $\theta'$. This makes our update $R + \gamma max_{a \in A}Q_{\theta'}(S_{t+1}, a) - Q_{\theta}(S_{t}, A_{t})$.
